{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Flights Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** FlightsDataExploration - The name of your Spark application, displayed in the Spark UI (e.g., `http://localhost:4040`).  Helps identify your application.\n",
    "\n",
    "+ **spark.driver.memory:** 1g - `Memory` allocated to the `Spark driver` process, which coordinates application execution. 1 gigabytes.\n",
    "\n",
    "+ **spark.executor.memory:** 1g - `Memory` allocated to each `Spark executor` process, where data processing and computations occur. 1 gigabytes.\n",
    "\n",
    "+ **spark.sql.shuffle.partitions:** 200 - Number of `partitions` created during `shuffle operations` (e.g., joins, aggregations). 200 partitions.\n",
    "\n",
    "+ **spark.sql.adaptive.enabled:** true - Enables `Adaptive Query Execution` (AQE) for dynamic query plan optimization. Enabled.\n",
    "\n",
    "+ **spark.sql.autoBroadcastJoinThreshold:** 100mb - `Size threshold` for a table to be considered for `broadcast join`. 100 megabytes.\n",
    "\n",
    "+ **spark.eventLog.enabled:** false - `Event Logging` on a spark history server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with configurations\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"FlightsDataExplorationControlShufflePartitions\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")  # Driver memory\n",
    "    .config(\"spark.executor.memory\", \"1g\")  # Executor memory\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"70\") # Control shuffle partitions\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")  # Enable adaptive query execution\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50mb\") # Adjust broadcast join threshold\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"40mb\") # Adjust max size per partition to control partition count\n",
    "    .config(\"spark.eventLog.enabled\", \"false\") # for some debug app will be disabled\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read\n",
    "\n",
    "This section focuses on reading the various datasets required for our flight analysis. We utilize the `spark.read.csv()` function to load data from CSV files into Spark DataFrames.  The `header=True` option specifies that the first row of each CSV file contains column names.  `inferSchema=True` tells Spark to attempt to automatically determine the data type of each column (e.g., string, integer, double).\n",
    "\n",
    "Specifically, we load the following datasets:\n",
    "\n",
    "* **Airlines Dataset:**  `airlines_df` contains information about airlines, including their IATA codes and full names.  It is read from the `airlines.csv` file.\n",
    "\n",
    "* **Airports Dataset:** `airports_df` contains details about airports, such as their IATA codes, names, city, state, and location.  It is read from the `airports.csv` file.\n",
    "\n",
    "* **Cancellation Codes Dataset:** `cancellation_codes_df` provides descriptions for different cancellation codes. It is read from the `cancellation_codes.csv` file.\n",
    "\n",
    "* **Flights Dataset:** `flights_df` contains detailed information about individual flights, including dates, times, airlines, origin and destination airports, delays, and cancellation status. It is read from the `flights.csv` file.\n",
    "\n",
    "These DataFrames will be used in subsequent sections for data exploration, transformations, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# --- Read Data ---\n",
    "airlines_df = spark.read \\\n",
    "  .csv(\n",
    "    \"data/flights/airlines.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    "  )\n",
    "airports_df = spark.read \\\n",
    "  .csv(\n",
    "    \"data/flights/airports.csv\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    "  )\n",
    "cancellation_codes_df = spark.read \\\n",
    "  .csv(\n",
    "    \"data/flights/cancellation_codes.csv\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Flight Data with Schema Inference**\n",
    "\n",
    "This code demonstrates an efficient way to read flight data from a CSV file using Spark, leveraging schema inference for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_flights_df = spark.read \\\n",
    "  .csv(\n",
    "    \"data/flights/flights.csv\",\n",
    "    header = True,\n",
    "    inferSchema = True\n",
    "  ) \\\n",
    "  .sample(\n",
    "    withReplacement = False,\n",
    "    fraction = 0.000001,\n",
    "    seed = 42\n",
    "  )\n",
    "\n",
    "flights_schema = sampled_flights_df.schema\n",
    "\n",
    "flights_df = spark.read \\\n",
    "  .csv(\n",
    "    \"data/flights/flights.csv\",\n",
    "    header = True,\n",
    "    schema=flights_schema\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Exploration and Transformations\n",
    "\n",
    "This section demonstrates various data exploration and transformation techniques using PySpark to analyze the flight data. We perform `aggregations`, `filtering`, `joins`, and other operations to gain insights into the data.  Because we are using a sample of the data, the results shown here will reflect the properties of the sample, not the full dataset.  If you wish to run these analyses on the full dataset, remove the `.sample()` operation and the `caching/persisting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which airline had the most cancellations?\n",
    "\n",
    "* **Finding the Airline with Most Cancellations:** We filter the `flights_df` to isolate cancelled flights (`CANCELLED` == 1). Then, we group the cancelled flights by `AIRLINE` and count the number of cancellations for each airline. The results are ordered in descending order of cancellation count to identify the airline with the most cancellations.\n",
    "\n",
    "* **Joining with Airline Names:** To provide more context, we join the cancellation counts with the `airlines_df` using the `AIRLINE` IATA code. This adds the full airline name to the cancellation counts, making the results easier to interpret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|AIRLINE|cancellation_count|\n",
      "+-------+------------------+\n",
      "|     WN|             16043|\n",
      "|     EV|             15231|\n",
      "|     MQ|             15025|\n",
      "|     AA|             10919|\n",
      "|     OO|              9960|\n",
      "|     UA|              6573|\n",
      "|     B6|              4276|\n",
      "|     US|              4067|\n",
      "|     DL|              3824|\n",
      "|     NK|              2004|\n",
      "|     AS|               669|\n",
      "|     F9|               588|\n",
      "|     VX|               534|\n",
      "|     HA|               171|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cancelled_flights = flights_df.filter(\n",
    "  col(\"CANCELLED\") == 1\n",
    ")\n",
    "\n",
    "cancellation_counts = (\n",
    "    cancelled_flights.groupBy(\"AIRLINE\")\n",
    "    .agg(\n",
    "      count(\"*\").alias(\"cancellation_count\")\n",
    "    )\n",
    "    .orderBy(\n",
    "      col(\"cancellation_count\").desc()\n",
    "    )\n",
    ")\n",
    "\n",
    "cancellation_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             AIRLINE|cancellation_count|\n",
      "+--------------------+------------------+\n",
      "|Southwest Airline...|             16043|\n",
      "|Delta Air Lines Inc.|              3824|\n",
      "|     JetBlue Airways|              4276|\n",
      "|American Airlines...|             10919|\n",
      "|American Eagle Ai...|             15025|\n",
      "|Skywest Airlines ...|              9960|\n",
      "|      Virgin America|               534|\n",
      "|United Air Lines ...|              6573|\n",
      "|Atlantic Southeas...|             15231|\n",
      "|    Spirit Air Lines|              2004|\n",
      "|Hawaiian Airlines...|               171|\n",
      "|Frontier Airlines...|               588|\n",
      "|     US Airways Inc.|              4067|\n",
      "|Alaska Airlines Inc.|               669|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join with airline names for better readability\n",
    "cancellation_counts_with_names = cancellation_counts.join(\n",
    "    airlines_df,\n",
    "    cancellation_counts[\"AIRLINE\"] == airlines_df[\"IATA_CODE\"],\n",
    "    \"inner\"\n",
    ").select(airlines_df[\"AIRLINE\"], \"cancellation_count\")\n",
    "\n",
    "cancellation_counts_with_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (12)\n",
      "+- Project (11)\n",
      "   +- BroadcastHashJoin Inner BuildRight (10)\n",
      "      :- HashAggregate (6)\n",
      "      :  +- Exchange (5)\n",
      "      :     +- HashAggregate (4)\n",
      "      :        +- Project (3)\n",
      "      :           +- Filter (2)\n",
      "      :              +- Scan csv  (1)\n",
      "      +- BroadcastExchange (9)\n",
      "         +- Filter (8)\n",
      "            +- Scan csv  (7)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [AIRLINE#798, CANCELLED#818]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/flights/flights.csv]\n",
      "PushedFilters: [IsNotNull(CANCELLED), EqualTo(CANCELLED,1), IsNotNull(AIRLINE)]\n",
      "ReadSchema: struct<AIRLINE:string,CANCELLED:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [AIRLINE#798, CANCELLED#818]\n",
      "Condition : ((isnotnull(CANCELLED#818) AND (CANCELLED#818 = 1)) AND isnotnull(AIRLINE#798))\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [AIRLINE#798]\n",
      "Input [2]: [AIRLINE#798, CANCELLED#818]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [1]: [AIRLINE#798]\n",
      "Keys [1]: [AIRLINE#798]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#898L]\n",
      "Results [2]: [AIRLINE#798, count#899L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [2]: [AIRLINE#798, count#899L]\n",
      "Arguments: hashpartitioning(AIRLINE#798, 70), ENSURE_REQUIREMENTS, [plan_id=518]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [AIRLINE#798, count#899L]\n",
      "Keys [1]: [AIRLINE#798]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#888L]\n",
      "Results [2]: [AIRLINE#798, count(1)#888L AS cancellation_count#889L]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [2]: [IATA_CODE#659, AIRLINE#660]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/flights/airlines.csv]\n",
      "PushedFilters: [IsNotNull(IATA_CODE)]\n",
      "ReadSchema: struct<IATA_CODE:string,AIRLINE:string>\n",
      "\n",
      "(8) Filter\n",
      "Input [2]: [IATA_CODE#659, AIRLINE#660]\n",
      "Condition : isnotnull(IATA_CODE#659)\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [2]: [IATA_CODE#659, AIRLINE#660]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=521]\n",
      "\n",
      "(10) BroadcastHashJoin\n",
      "Left keys [1]: [AIRLINE#798]\n",
      "Right keys [1]: [IATA_CODE#659]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [AIRLINE#660, cancellation_count#889L]\n",
      "Input [4]: [AIRLINE#798, cancellation_count#889L, IATA_CODE#659, AIRLINE#660]\n",
      "\n",
      "(12) AdaptiveSparkPlan\n",
      "Output [2]: [AIRLINE#660, cancellation_count#889L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 12:48:16 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.144.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/02/26 12:48:16 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.144.5: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/02/26 12:48:16 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.144.4: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "cancellation_counts_with_names.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are top 10 Airlines with most flights:**\n",
    "\n",
    "* **Counting Flights per Airline:** We group the `flights_df` by `AIRLINE` and count the number of flights for each airline.\n",
    "\n",
    "* **Limiting to Top 10:** We use the `limit()` function to select only the top 10 airlines with the most flights.\n",
    "\n",
    "* **Ordering and Displaying Results:** The results are ordered by flight count in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|AIRLINE|flight_count|\n",
      "+-------+------------+\n",
      "|     WN|     1261855|\n",
      "|     DL|      875881|\n",
      "|     AA|      725984|\n",
      "|     OO|      588353|\n",
      "|     EV|      571977|\n",
      "|     UA|      515723|\n",
      "|     MQ|      294632|\n",
      "|     B6|      267048|\n",
      "|     US|      198715|\n",
      "|     AS|      172521|\n",
      "+-------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_airlines = (\n",
    "    flights_df.groupBy(\"AIRLINE\")\n",
    "    .agg(count(\"*\").alias(\"flight_count\"))\n",
    "    .orderBy(col(\"flight_count\").desc())\n",
    "    .limit(10)  # Limit to top 10\n",
    ")\n",
    "top_airlines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What were the top 10 busiest airports (most flights)?\n",
    "\n",
    "* **Calculating Airport Traffic:** We determine the busiest airports by calculating the total number of flights (arrivals and departures) for each airport. We group the `flights_df` by `ORIGIN_AIRPORT` and `DESTINATION_AIRPORT` separately to count departures and arrivals.\n",
    "\n",
    "* **Combining Arrival and Departure Counts:** We perform a full outer join on the origin and destination counts to get the combined traffic for each airport.  The `coalesce` function is used to handle cases where an airport might only have arrivals or departures, ensuring that all airports are included in the results. The total traffic is calculated by summing the origin and destination counts.\n",
    "\n",
    "* **Ordering and Displaying Results:** The results are ordered by total traffic in descending order to show the busiest airports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_counts = flights_df.groupBy(\"ORIGIN_AIRPORT\") \\\n",
    "  .agg(\n",
    "    count(\"*\")\n",
    "    .alias(\"origin_count\")\n",
    "  )\n",
    "destination_counts = flights_df.groupBy(\"DESTINATION_AIRPORT\") \\\n",
    "  .agg(\n",
    "    count(\"*\")\n",
    "    .alias(\"destination_count\")\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['total_traffic DESC NULLS LAST], true\n",
      "+- Project [airport#325, origin_count#326L, destination_count#327L, (origin_count#326L + destination_count#327L) AS total_traffic#331L]\n",
      "   +- Project [coalesce(ORIGIN_AIRPORT#159, DESTINATION_AIRPORT#293) AS airport#325, coalesce(origin_count#246L, cast(0 as bigint)) AS origin_count#326L, coalesce(destination_count#282L, cast(0 as bigint)) AS destination_count#327L]\n",
      "      +- Join FullOuter, (ORIGIN_AIRPORT#159 = DESTINATION_AIRPORT#293)\n",
      "         :- Aggregate [ORIGIN_AIRPORT#159], [ORIGIN_AIRPORT#159, count(1) AS origin_count#246L]\n",
      "         :  +- Relation [YEAR#152,MONTH#153,DAY#154,DAY_OF_WEEK#155,AIRLINE#156,FLIGHT_NUMBER#157,TAIL_NUMBER#158,ORIGIN_AIRPORT#159,DESTINATION_AIRPORT#160,SCHEDULED_DEPARTURE#161,DEPARTURE_TIME#162,DEPARTURE_DELAY#163,TAXI_OUT#164,WHEELS_OFF#165,SCHEDULED_TIME#166,ELAPSED_TIME#167,AIR_TIME#168,DISTANCE#169,WHEELS_ON#170,TAXI_IN#171,SCHEDULED_ARRIVAL#172,ARRIVAL_TIME#173,ARRIVAL_DELAY#174,DIVERTED#175,... 7 more fields] csv\n",
      "         +- Aggregate [DESTINATION_AIRPORT#293], [DESTINATION_AIRPORT#293, count(1) AS destination_count#282L]\n",
      "            +- Relation [YEAR#285,MONTH#286,DAY#287,DAY_OF_WEEK#288,AIRLINE#289,FLIGHT_NUMBER#290,TAIL_NUMBER#291,ORIGIN_AIRPORT#292,DESTINATION_AIRPORT#293,SCHEDULED_DEPARTURE#294,DEPARTURE_TIME#295,DEPARTURE_DELAY#296,TAXI_OUT#297,WHEELS_OFF#298,SCHEDULED_TIME#299,ELAPSED_TIME#300,AIR_TIME#301,DISTANCE#302,WHEELS_ON#303,TAXI_IN#304,SCHEDULED_ARRIVAL#305,ARRIVAL_TIME#306,ARRIVAL_DELAY#307,DIVERTED#308,... 7 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "airport: string, origin_count: bigint, destination_count: bigint, total_traffic: bigint\n",
      "Sort [total_traffic#331L DESC NULLS LAST], true\n",
      "+- Project [airport#325, origin_count#326L, destination_count#327L, (origin_count#326L + destination_count#327L) AS total_traffic#331L]\n",
      "   +- Project [coalesce(ORIGIN_AIRPORT#159, DESTINATION_AIRPORT#293) AS airport#325, coalesce(origin_count#246L, cast(0 as bigint)) AS origin_count#326L, coalesce(destination_count#282L, cast(0 as bigint)) AS destination_count#327L]\n",
      "      +- Join FullOuter, (ORIGIN_AIRPORT#159 = DESTINATION_AIRPORT#293)\n",
      "         :- Aggregate [ORIGIN_AIRPORT#159], [ORIGIN_AIRPORT#159, count(1) AS origin_count#246L]\n",
      "         :  +- Relation [YEAR#152,MONTH#153,DAY#154,DAY_OF_WEEK#155,AIRLINE#156,FLIGHT_NUMBER#157,TAIL_NUMBER#158,ORIGIN_AIRPORT#159,DESTINATION_AIRPORT#160,SCHEDULED_DEPARTURE#161,DEPARTURE_TIME#162,DEPARTURE_DELAY#163,TAXI_OUT#164,WHEELS_OFF#165,SCHEDULED_TIME#166,ELAPSED_TIME#167,AIR_TIME#168,DISTANCE#169,WHEELS_ON#170,TAXI_IN#171,SCHEDULED_ARRIVAL#172,ARRIVAL_TIME#173,ARRIVAL_DELAY#174,DIVERTED#175,... 7 more fields] csv\n",
      "         +- Aggregate [DESTINATION_AIRPORT#293], [DESTINATION_AIRPORT#293, count(1) AS destination_count#282L]\n",
      "            +- Relation [YEAR#285,MONTH#286,DAY#287,DAY_OF_WEEK#288,AIRLINE#289,FLIGHT_NUMBER#290,TAIL_NUMBER#291,ORIGIN_AIRPORT#292,DESTINATION_AIRPORT#293,SCHEDULED_DEPARTURE#294,DEPARTURE_TIME#295,DEPARTURE_DELAY#296,TAXI_OUT#297,WHEELS_OFF#298,SCHEDULED_TIME#299,ELAPSED_TIME#300,AIR_TIME#301,DISTANCE#302,WHEELS_ON#303,TAXI_IN#304,SCHEDULED_ARRIVAL#305,ARRIVAL_TIME#306,ARRIVAL_DELAY#307,DIVERTED#308,... 7 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [total_traffic#331L DESC NULLS LAST], true\n",
      "+- Project [airport#325, origin_count#326L, destination_count#327L, (origin_count#326L + destination_count#327L) AS total_traffic#331L]\n",
      "   +- Project [coalesce(ORIGIN_AIRPORT#159, DESTINATION_AIRPORT#293) AS airport#325, coalesce(origin_count#246L, 0) AS origin_count#326L, coalesce(destination_count#282L, 0) AS destination_count#327L]\n",
      "      +- Join FullOuter, (ORIGIN_AIRPORT#159 = DESTINATION_AIRPORT#293)\n",
      "         :- Aggregate [ORIGIN_AIRPORT#159], [ORIGIN_AIRPORT#159, count(1) AS origin_count#246L]\n",
      "         :  +- Project [ORIGIN_AIRPORT#159]\n",
      "         :     +- Relation [YEAR#152,MONTH#153,DAY#154,DAY_OF_WEEK#155,AIRLINE#156,FLIGHT_NUMBER#157,TAIL_NUMBER#158,ORIGIN_AIRPORT#159,DESTINATION_AIRPORT#160,SCHEDULED_DEPARTURE#161,DEPARTURE_TIME#162,DEPARTURE_DELAY#163,TAXI_OUT#164,WHEELS_OFF#165,SCHEDULED_TIME#166,ELAPSED_TIME#167,AIR_TIME#168,DISTANCE#169,WHEELS_ON#170,TAXI_IN#171,SCHEDULED_ARRIVAL#172,ARRIVAL_TIME#173,ARRIVAL_DELAY#174,DIVERTED#175,... 7 more fields] csv\n",
      "         +- Aggregate [DESTINATION_AIRPORT#293], [DESTINATION_AIRPORT#293, count(1) AS destination_count#282L]\n",
      "            +- Project [DESTINATION_AIRPORT#293]\n",
      "               +- Relation [YEAR#285,MONTH#286,DAY#287,DAY_OF_WEEK#288,AIRLINE#289,FLIGHT_NUMBER#290,TAIL_NUMBER#291,ORIGIN_AIRPORT#292,DESTINATION_AIRPORT#293,SCHEDULED_DEPARTURE#294,DEPARTURE_TIME#295,DEPARTURE_DELAY#296,TAXI_OUT#297,WHEELS_OFF#298,SCHEDULED_TIME#299,ELAPSED_TIME#300,AIR_TIME#301,DISTANCE#302,WHEELS_ON#303,TAXI_IN#304,SCHEDULED_ARRIVAL#305,ARRIVAL_TIME#306,ARRIVAL_DELAY#307,DIVERTED#308,... 7 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_traffic#331L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_traffic#331L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=126]\n",
      "      +- Project [airport#325, origin_count#326L, destination_count#327L, (origin_count#326L + destination_count#327L) AS total_traffic#331L]\n",
      "         +- Project [coalesce(ORIGIN_AIRPORT#159, DESTINATION_AIRPORT#293) AS airport#325, coalesce(origin_count#246L, 0) AS origin_count#326L, coalesce(destination_count#282L, 0) AS destination_count#327L]\n",
      "            +- SortMergeJoin [ORIGIN_AIRPORT#159], [DESTINATION_AIRPORT#293], FullOuter\n",
      "               :- Sort [ORIGIN_AIRPORT#159 ASC NULLS FIRST], false, 0\n",
      "               :  +- HashAggregate(keys=[ORIGIN_AIRPORT#159], functions=[count(1)], output=[ORIGIN_AIRPORT#159, origin_count#246L])\n",
      "               :     +- Exchange hashpartitioning(ORIGIN_AIRPORT#159, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "               :        +- HashAggregate(keys=[ORIGIN_AIRPORT#159], functions=[partial_count(1)], output=[ORIGIN_AIRPORT#159, count#337L])\n",
      "               :           +- FileScan csv [ORIGIN_AIRPORT#159] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/workspace/data/flights/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_AIRPORT:string>\n",
      "               +- Sort [DESTINATION_AIRPORT#293 ASC NULLS FIRST], false, 0\n",
      "                  +- HashAggregate(keys=[DESTINATION_AIRPORT#293], functions=[count(1)], output=[DESTINATION_AIRPORT#293, destination_count#282L])\n",
      "                     +- Exchange hashpartitioning(DESTINATION_AIRPORT#293, 200), ENSURE_REQUIREMENTS, [plan_id=116]\n",
      "                        +- HashAggregate(keys=[DESTINATION_AIRPORT#293], functions=[partial_count(1)], output=[DESTINATION_AIRPORT#293, count#339L])\n",
      "                           +- FileScan csv [DESTINATION_AIRPORT#293] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/workspace/data/flights/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DESTINATION_AIRPORT:string>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 12:22:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "airport_traffic = origin_counts.join(\n",
    "  destination_counts,\n",
    "  origin_counts[\"ORIGIN_AIRPORT\"] == destination_counts[\"DESTINATION_AIRPORT\"],\n",
    "  \"fullouter\"\n",
    "  ) \\\n",
    "  .select(\n",
    "    coalesce(\n",
    "      col(\"ORIGIN_AIRPORT\"),\n",
    "      col(\"DESTINATION_AIRPORT\")\n",
    "    ).alias(\"airport\"),\n",
    "    coalesce(\n",
    "      origin_counts[\"origin_count\"],\n",
    "      lit(0)\n",
    "    ).alias(\"origin_count\"),\n",
    "    coalesce(\n",
    "      destination_counts[\"destination_count\"],\n",
    "      lit(0)\n",
    "    ).alias(\"destination_count\")) \\\n",
    "  .withColumn(\n",
    "    \"total_traffic\",\n",
    "    col(\"origin_count\") + col(\"destination_count\")\n",
    "  ) \\\n",
    "  .orderBy(\n",
    "    col(\"total_traffic\").desc()\n",
    "  ).explain(True)\n",
    "\n",
    "# airport_traffic.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
