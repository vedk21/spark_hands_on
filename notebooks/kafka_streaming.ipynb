{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kafka Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports & Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, split, col, current_timestamp, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using packages ['org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0', 'org.apache.kafka:kafka-clients:3.5.1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = ''\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"KafkaStreamingAnalysis\") \\\n",
    "  .config(\"spark.executor.cores\", \"2\") \\\n",
    "  .config(\"spark.executor.memory\", \"1500m\") \\\n",
    "  .config(\"spark.driver.memory\", \"1500m\") \\\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "  .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10mb\") \\\n",
    "  .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "  .config(\"spark.sql.files.maxPartitionBytes\", \"1mb\") \\\n",
    "  .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "  .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "  .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "  StructField(\"customer_id\", IntegerType()),\n",
    "  StructField(\"name\", StringType()),\n",
    "  StructField(\"email\", StringType()),\n",
    "  StructField(\"registration_date\", StringType())\n",
    "])\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"order_date\", StringType()),\n",
    "    StructField(\"total_amount\", FloatType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_brokers = 'broker-1:29091,broker-2:29092,broker-3:29093'\n",
    "customers_topic_name = \"customers\"\n",
    "orders_topic_name = \"orders\"\n",
    "\n",
    "# -- Read customer Data --\n",
    "customer_stream_df = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "  .option(\"subscribe\", customers_topic_name) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "customer_parsed_df = customer_stream_df.selectExpr(\"CAST(value AS String)\") \\\n",
    "  .select(from_json(col(\"value\"), customer_schema).alias(\"customer_data\")) \\\n",
    "  .select(\"customer_data.*\") \\\n",
    "  .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "# -- Read Order Data\n",
    "order_stream_df = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "  .option(\"subscribe\", orders_topic_name) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "order_parsed_df = order_stream_df.selectExpr(\"CAST(value AS String)\") \\\n",
    "  .select(from_json(col(\"value\"), order_schema).alias(\"order_data\")) \\\n",
    "  .select(\"order_data.*\") \\\n",
    "  .withColumn(\"ingestion_timestamp\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Validation & Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Convert the datetime/timestamp into correct formats --\n",
    "customer_corrected_df = customer_parsed_df.withColumn(\n",
    "  \"registration_timestamp\",\n",
    "  to_timestamp(\n",
    "    col(\"registration_date\"),\n",
    "    \"yyyy-MM-dd HH:mm:ss\"\n",
    "  )\n",
    ").drop(\"registration_date\")\n",
    "\n",
    "order_corrected_df = order_parsed_df.withColumn(\n",
    "  \"order_timestamp\",\n",
    "  to_timestamp(\n",
    "    col(\"order_date\"),\n",
    "    \"yyyy-MM-dd HH:mm:ss\"\n",
    "  )\n",
    ").drop(\"order_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write Stream To Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_customers_local_dir = \"data/customers\"\n",
    "base_orders_local_dir = \"data/orders\"\n",
    "\n",
    "base_customers_checkpoint_dir = \"data/checkpoints/customers\"\n",
    "base_orders_checkpoint_dir = \"data/checkpoints/orders\"\n",
    "\n",
    "customer_query = customer_corrected_df.writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"path\", f\"{base_customers_local_dir}\") \\\n",
    "  .option(\"checkpointLocation\", f\"{base_customers_checkpoint_dir}\") \\\n",
    "  .trigger(processingTime='10 seconds') \\\n",
    "  .start()\n",
    "\n",
    "order_query = order_corrected_df.writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"path\", f\"{base_orders_local_dir}\") \\\n",
    "  .option(\"checkpointLocation\", f\"{base_orders_checkpoint_dir}\") \\\n",
    "  .trigger(processingTime='10 seconds') \\\n",
    "  .start()\n",
    "\n",
    "customer_query.awaitTermination()\n",
    "order_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Stop Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_query.stop()\n",
    "order_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
